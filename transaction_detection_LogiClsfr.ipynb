{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_addr_addr = pd.read_csv('data/AddrAddr_edgelist.csv')\n",
    "df_wallets_features = pd.read_csv('data/wallets_features_classes_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with class = 3\n",
    "df_wallets_features = df_wallets_features[df_wallets_features['class'] != 3]\n",
    "\n",
    "# change class 1 to 0 and class 2 to 1 - this is needed for binary classification\n",
    "df_wallets_features['class'] = df_wallets_features['class'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# order by Time step from smallest to largest\n",
    "df_wallets_features = df_wallets_features.sort_values('Time step', ascending=True)\n",
    "\n",
    "# drop Time step\n",
    "df_wallets_features.drop('Time step', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if there are any duplicates in the data that can cause problems... if its classes are different from time to time\n",
    "# super_dict = {}\n",
    "# reoccurences = 0\n",
    "# problems = []\n",
    "# for i, row in tqdm(df_wallets_features.iterrows(), total=len(df_wallets_features), desc='Checking wallet classes'):\n",
    "#     addr = row['address']\n",
    "#     class_ = row['class']\n",
    "#     if addr in super_dict:\n",
    "#         reoccurences += 1\n",
    "#         if super_dict[addr] != class_:\n",
    "#             print('Address with different classes:', addr)\n",
    "#             problems.append(addr)\n",
    "#     else: \n",
    "#         super_dict[addr] = class_\n",
    "# print('Number of unique addresses in wallet features:', len(super_dict))\n",
    "# print('Number of reoccurences:', reoccurences)\n",
    "# if problems:\n",
    "#     print('Problems:', problems)\n",
    "# else: \n",
    "#     print('No problems found...')\n",
    "# assert len(super_dict) + reoccurences == len(df_wallets_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove reoccurences, keep the last one\n",
    "df_wallets_features = df_wallets_features.drop_duplicates(subset='address', keep='last')\n",
    "df_wallets_features = df_wallets_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want to keep the edges if both input and output addresses are in the wallet features\n",
    "df_addr_addr = df_addr_addr[df_addr_addr['input_address'].isin(df_wallets_features['address'])]\n",
    "df_addr_addr = df_addr_addr[df_addr_addr['output_address'].isin(df_wallets_features['address'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized all columns except the address and class (index 0 and 1)\n",
    "df_wallets_features.iloc[:, 2:] = df_wallets_features.iloc[:, 2:].apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded.\n",
      "Number of nodes: 265354\n",
      "Number of edges: 1090054\n"
     ]
    }
   ],
   "source": [
    "# create a graph from the edges dataframe\n",
    "G = nx.from_pandas_edgelist(df_addr_addr, 'input_address', 'output_address')\n",
    "\n",
    "# add isolated nodes to the graph\n",
    "for address in df_wallets_features['address']:\n",
    "    if address not in G:\n",
    "        G.add_node(address)\n",
    "\n",
    "print('Graph loaded.')\n",
    "print('Number of nodes:', G.number_of_nodes())\n",
    "print('Number of edges:', G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding node features: 100%|██████████| 265354/265354 [00:07<00:00, 35328.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import from_networkx  # type: ignore\n",
    "\n",
    "# add node features and class labels to the graph\n",
    "for _, row in tqdm(\n",
    "    df_wallets_features.iterrows(),\n",
    "    total=len(df_wallets_features),\n",
    "    desc=\"Adding node features\",\n",
    "):\n",
    "    G.nodes[row[\"address\"]].update(row.to_dict())\n",
    "# convert the NetworkX graph to PyTorch Geometric data\n",
    "data = from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "\n",
    "# def evaluate(mask, model, data, class_weights):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         out = model(data)\n",
    "#         pred = out[mask].max(dim=1)[1]\n",
    "#         true_labels = data.y[mask].cpu()\n",
    "#         pred_labels = pred.cpu()\n",
    "\n",
    "#         # Calculate F1 scores for both classes\n",
    "#         recall_0 = recall_score(true_labels, pred_labels, pos_label=0)\n",
    "#         precision_0 = precision_score(true_labels, pred_labels, pos_label=0)\n",
    "#         f1_0 = f1_score(true_labels, pred_labels, pos_label=0)\n",
    "\n",
    "#         recall_1 = recall_score(true_labels, pred_labels, pos_label=1)\n",
    "#         precision_1 = precision_score(true_labels, pred_labels, pos_label=1)\n",
    "#         f1_1 = f1_score(true_labels, pred_labels, pos_label=1)\n",
    "\n",
    "#         # Calculate weighted average F1 score\n",
    "#         weighted_f1 = class_weights[0] * f1_0 + class_weights[1] * f1_1\n",
    "#         r = 0.75\n",
    "#         weighted_f1 = r * f1_0 + (1 - r) * f1_1\n",
    "\n",
    "#     return (recall_0, precision_0, f1_0), (recall_1, precision_1, f1_1), weighted_f1\n",
    "\n",
    "\n",
    "def evaluate(mask, model, data, _):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out[mask].max(dim=1)[1]\n",
    "        true_labels = data.y[mask].cpu()\n",
    "        pred_labels = pred.cpu()\n",
    "        recall = recall_score(true_labels, pred_labels, pos_label=0)\n",
    "        precision = precision_score(true_labels, pred_labels, pos_label=0)\n",
    "        f1 = f1_score(true_labels, pred_labels, pos_label=0)\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_GAT import GAT\n",
    "from Model_GCN import GCN\n",
    "from Model_TAGNN import TAGNN\n",
    "\n",
    "\n",
    "# Update setup_model function\n",
    "def setup_model(\n",
    "    model_class,\n",
    "    input_channels,\n",
    "    hidden_channels_1,\n",
    "    hidden_channels_2,\n",
    "    output_channels,\n",
    "    dropout,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    gat_heads=1,\n",
    "    data=None,\n",
    "):\n",
    "    # Dynamically create the model based on the model_class\n",
    "    if model_class == GAT:\n",
    "        model = GAT(\n",
    "            input_channels,\n",
    "            hidden_channels_1,\n",
    "            hidden_channels_2,\n",
    "            output_channels,\n",
    "            dropout,\n",
    "            heads=gat_heads,\n",
    "        )\n",
    "    elif model_class == TAGNN:\n",
    "        model = TAGNN(\n",
    "            input_channels,\n",
    "            hidden_channels_1,\n",
    "            hidden_channels_2,\n",
    "            output_channels,\n",
    "            dropout,\n",
    "        )\n",
    "    elif model_class == GCN:\n",
    "        model = GCN(\n",
    "            input_channels,\n",
    "            hidden_channels_1,\n",
    "            hidden_channels_2,\n",
    "            output_channels,\n",
    "            dropout,\n",
    "        )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "    if data is None:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        # Adjust the loss function to give more importance to the minority class\n",
    "        # Recount the classes after augmenting\n",
    "        augmented_class_counts = data.y[data.train_mask].bincount().cpu().numpy()\n",
    "\n",
    "        # Adjust the loss function to give more importance to the minority class\n",
    "        class_weights = torch.tensor([augmented_class_counts[1] / augmented_class_counts[0], 1.0], dtype=torch.float).to(\"cuda\")\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model, data, optimizer, loss_fn, epochs, verbose=False):\n",
    "    best_f1 = 0\n",
    "    best_model_iter = 0\n",
    "    best_model = None\n",
    "    class_counts = data.y[data.train_mask].bincount().cpu().numpy()\n",
    "    total_samples = sum(class_counts)\n",
    "    class_weights = class_counts / total_samples\n",
    "\n",
    "    if verbose:\n",
    "        for epoch in range(epochs):\n",
    "            loss = train(model, data, optimizer, loss_fn)\n",
    "            _, _, train_f1 = evaluate(data.train_mask, model, data, class_weights)\n",
    "            _, _, val_f1 = evaluate(data.val_mask, model, data, class_weights)\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                best_model_iter = epoch\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1}, Loss: {loss:.4f}, Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}\"\n",
    "                )\n",
    "    else:\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            loss = train(model, data, optimizer, loss_fn)\n",
    "            _, _, train_f1 = evaluate(data.train_mask, model, data, class_weights)\n",
    "            _, _, val_f1 = evaluate(data.val_mask, model, data, class_weights)\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                best_model_iter = epoch\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Loading the best model at iteration {best_model_iter}\")\n",
    "    model.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "def test_model(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out[data.test_mask].max(dim=1)[1]\n",
    "        correct = pred.eq(data.y[data.test_mask]).sum().item()\n",
    "        acc = correct / data.test_mask.sum().item()\n",
    "        print(f\"Test Accuracy: {acc:.4f}\")\n",
    "        print(confusion_matrix(data.y[data.test_mask].cpu(), pred.cpu()))\n",
    "        print(classification_report(data.y[data.test_mask].cpu(), pred.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed hyperparameters\n",
    "EPOCH = 300\n",
    "INPUT_CHANNELS = 55\n",
    "OUTPUT_CHANNELS = 2\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "# tunable hyperparameters\n",
    "LEARNING_RATE = 0.01\n",
    "HIDDEN_CHANNELS_1_GCN, HIDDEN_CHANNELS_1_GAT, HIDDEN_CHANNELS_1_TAGNN = 32, 16, 32\n",
    "HIDDEN_CHANNELS_2_GCN, HIDDEN_CHANNELS_2_GAT, HIDDEN_CHANNELS_2_TAGNN = 16, 24, 16\n",
    "DROPOUT = 0.5\n",
    "GAT_HEADS = 2\n",
    "SAMPLING_STRATEGY = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in the new training set: [105456 175761]\n",
      "Class distribution in validation set: [ 1427 25108]\n",
      "Class distribution in test set: [ 2853 50219]\n"
     ]
    }
   ],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def resample(df_wallets_features, sampling_strategy, verbose=False):\n",
    "#     # Prepare the feature matrix and labels\n",
    "#     feature_columns = [\n",
    "#         col for col in df_wallets_features.columns if col not in [\"address\", \"class\"]\n",
    "#     ]\n",
    "#     data.x = torch.tensor(\n",
    "#         df_wallets_features[feature_columns].values, dtype=torch.float\n",
    "#     )\n",
    "#     data.y = torch.tensor(df_wallets_features[\"class\"].values, dtype=torch.long)\n",
    "\n",
    "#     # Split the data into training, validation, and test sets\n",
    "#     train_ratio = 0.7\n",
    "#     val_ratio = 0.1\n",
    "#     test_ratio = 0.2\n",
    "#     assert train_ratio + val_ratio + test_ratio == 1.0\n",
    "#     assert 0 < sampling_strategy <= 1\n",
    "\n",
    "#     # Split indices into training and temp sets\n",
    "#     train_idx, temp_idx, train_labels, temp_labels = train_test_split(\n",
    "#         np.arange(len(data.y)),\n",
    "#         data.y,\n",
    "#         stratify=data.y,\n",
    "#         test_size=(1 - train_ratio),\n",
    "#         random_state=42,\n",
    "#     )\n",
    "\n",
    "#     # Split temp set into validation and test sets\n",
    "#     val_idx, test_idx, val_labels, test_labels = train_test_split(\n",
    "#         temp_idx,\n",
    "#         temp_labels,\n",
    "#         stratify=temp_labels,\n",
    "#         test_size=(test_ratio / (val_ratio + test_ratio)),\n",
    "#         random_state=42,\n",
    "#     )\n",
    "\n",
    "#     # Create boolean masks\n",
    "#     train_mask = torch.zeros(len(data.y), dtype=torch.bool)\n",
    "#     val_mask = torch.zeros(len(data.y), dtype=torch.bool)\n",
    "#     test_mask = torch.zeros(len(data.y), dtype=torch.bool)\n",
    "\n",
    "#     train_mask[train_idx] = True\n",
    "#     val_mask[val_idx] = True\n",
    "#     test_mask[test_idx] = True\n",
    "\n",
    "#     # Apply SMOTE to handle class imbalance only on the training set\n",
    "#     smote = SMOTE(sampling_strategy=SAMPLING_STRATEGY, random_state=42)\n",
    "#     X_resampled, y_resampled = smote.fit_resample(\n",
    "#         data.x[train_mask].numpy(), data.y[train_mask].numpy()\n",
    "#     )\n",
    "\n",
    "#     # Create new tensors for resampled training features and labels\n",
    "#     new_train_features = torch.tensor(X_resampled, dtype=torch.float)\n",
    "#     new_train_labels = torch.tensor(y_resampled, dtype=torch.long)\n",
    "\n",
    "#     # Combine resampled training data with the original validation and test data\n",
    "#     combined_features = torch.cat(\n",
    "#         (new_train_features, data.x[val_mask], data.x[test_mask]), dim=0\n",
    "#     )\n",
    "#     combined_labels = torch.cat(\n",
    "#         (new_train_labels, data.y[val_mask], data.y[test_mask]), dim=0\n",
    "#     )\n",
    "\n",
    "#     # Update indices for the combined dataset\n",
    "#     new_train_idx = torch.arange(len(new_train_labels))\n",
    "#     new_val_idx = torch.arange(\n",
    "#         len(new_train_labels), len(new_train_labels) + len(data.y[val_mask])\n",
    "#     )\n",
    "#     new_test_idx = torch.arange(\n",
    "#         len(new_train_labels) + len(data.y[val_mask]), len(combined_labels)\n",
    "#     )\n",
    "\n",
    "#     # Create new boolean masks for the combined dataset\n",
    "#     new_train_mask = torch.zeros(len(combined_labels), dtype=torch.bool)\n",
    "#     new_val_mask = torch.zeros_like(new_train_mask)\n",
    "#     new_test_mask = torch.zeros_like(new_train_mask)\n",
    "\n",
    "#     new_train_mask[new_train_idx] = True\n",
    "#     new_val_mask[new_val_idx] = True\n",
    "#     new_test_mask[new_test_idx] = True\n",
    "\n",
    "#     # Update the data object\n",
    "#     data.x = combined_features\n",
    "#     data.y = combined_labels\n",
    "#     data.train_mask = new_train_mask\n",
    "#     data.val_mask = new_val_mask\n",
    "#     data.test_mask = new_test_mask\n",
    "\n",
    "#     # Verify the class distribution in the new training set\n",
    "#     if verbose:\n",
    "#         print(\n",
    "#             \"Class distribution in the new training set:\",\n",
    "#             data.y[data.train_mask].bincount().cpu().numpy(),\n",
    "#         )\n",
    "#         print(\n",
    "#             \"Class distribution in validation set:\",\n",
    "#             data.y[data.val_mask].bincount().cpu().numpy(),\n",
    "#         )\n",
    "#         print(\n",
    "#             \"Class distribution in test set:\",\n",
    "#             data.y[data.test_mask].bincount().cpu().numpy(),\n",
    "#         )\n",
    "\n",
    "#     return data\n",
    "\n",
    "# set_seed(3407)\n",
    "# data = resample(df_wallets_features, SAMPLING_STRATEGY, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# # gcn_model, gcn_optimizer, gcn_loss_fn = setup_model(\n",
    "# #     GCN,\n",
    "# #     INPUT_CHANNELS,\n",
    "# #     HIDDEN_CHANNELS_1_GCN,\n",
    "# #     HIDDEN_CHANNELS_2_GCN,\n",
    "# #     OUTPUT_CHANNELS,\n",
    "# #     DROPOUT,\n",
    "# #     LEARNING_RATE,\n",
    "# #     WEIGHT_DECAY,\n",
    "# #     data=data,\n",
    "# # )\n",
    "# set_seed(3407)\n",
    "# gat_model, gat_optimizer, gat_loss_fn = setup_model(\n",
    "#     GAT,\n",
    "#     INPUT_CHANNELS,\n",
    "#     HIDDEN_CHANNELS_1_GAT,\n",
    "#     HIDDEN_CHANNELS_2_GAT,\n",
    "#     OUTPUT_CHANNELS,\n",
    "#     DROPOUT,\n",
    "#     LEARNING_RATE,\n",
    "#     WEIGHT_DECAY,\n",
    "#     GAT_HEADS,\n",
    "#     data=data,\n",
    "# )\n",
    "# # tagnn_model, tagnn_optimizer, tagnn_loss_fn = setup_model(\n",
    "# #     TAGNN,\n",
    "# #     INPUT_CHANNELS,\n",
    "# #     HIDDEN_CHANNELS_1_TAGNN,\n",
    "# #     HIDDEN_CHANNELS_2_TAGNN,\n",
    "# #     OUTPUT_CHANNELS,\n",
    "# #     DROPOUT,\n",
    "# #     LEARNING_RATE,\n",
    "# #     WEIGHT_DECAY,\n",
    "# #     data=data,\n",
    "# # )\n",
    "\n",
    "# data.to(\"cuda\")\n",
    "# # gcn_model.to(\"cuda\")\n",
    "# gat_model.to(\"cuda\")\n",
    "# # tagnn_model.to(\"cuda\")\n",
    "\n",
    "# # train_model(gcn_model, data, gcn_optimizer, gcn_loss_fn, EPOCH)\n",
    "# # test_model(gcn_model, data)\n",
    "# set_seed(3407)\n",
    "# train_model(gat_model, data, gat_optimizer, gat_loss_fn, EPOCH)\n",
    "\n",
    "# test_model(gat_model, data)\n",
    "# # train_model(tagnn_model, data, tagnn_optimizer, tagnn_loss_fn, EPOCH)\n",
    "# # test_model(tagnn_model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Graph-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in the new training set: [105456 175761]\n",
      "Class distribution in validation set: [ 1427 25108]\n",
      "Class distribution in test set: [ 2853 50219]\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Prepare the feature matrix and labels\n",
    "feature_columns = [col for col in df_wallets_features.columns if col not in ['address', 'class']]\n",
    "X = df_wallets_features[feature_columns].values\n",
    "y = df_wallets_features['class'].values\n",
    "\n",
    "# Split data into training and temp sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_ratio), stratify=y, random_state=42)\n",
    "\n",
    "# Split temp set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_ratio / (val_ratio + test_ratio)), stratify=y_temp, random_state=42)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance only on the training set\n",
    "smote = SMOTE(sampling_strategy=SAMPLING_STRATEGY, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Update training data with resampled data\n",
    "X_train = X_train_resampled\n",
    "y_train = y_train_resampled\n",
    "\n",
    "# Verify the class distribution in the new training set\n",
    "print(\"Class distribution in the new training set:\", np.bincount(y_train))\n",
    "print(\"Class distribution in validation set:\", np.bincount(y_val))\n",
    "print(\"Class distribution in test set:\", np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:07<00:00, 27.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the best model at iteration 12\n",
      "Test Accuracy: 0.9143\n",
      "Test F1 Score (minority class): 0.4344\n",
      "[[ 1747  1106]\n",
      " [ 3444 46775]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.61      0.43      2853\n",
      "           1       0.98      0.93      0.95     50219\n",
      "\n",
      "    accuracy                           0.91     53072\n",
      "   macro avg       0.66      0.77      0.69     53072\n",
      "weighted avg       0.94      0.91      0.93     53072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Model_Linear_Classification import LinearClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    ")\n",
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 55  # Number of input features\n",
    "hidden_dim1 = 32\n",
    "hidden_dim2 = 16\n",
    "output_dim = 2  # Binary classification\n",
    "dropout = 0.5\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "\n",
    "set_seed(3407)\n",
    "\n",
    "# Setup\n",
    "model = LinearClassifier(input_dim, hidden_dim1, hidden_dim2, output_dim, dropout)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "best_val_f1 = 0\n",
    "best_model_iter = 0\n",
    "best_model = None\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_output = model(X_val_tensor)\n",
    "        val_pred = val_output.argmax(dim=1)\n",
    "        val_f1 = f1_score(\n",
    "            y_val_tensor, val_pred, pos_label=0\n",
    "        )  # F1 score for minority class\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_model_iter = epoch\n",
    "            best_val_f1 = val_f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# Load the best model\n",
    "print(f\"Loading the best model at iteration {best_model_iter}\")\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(X_test_tensor)\n",
    "    test_pred = test_output.argmax(dim=1)\n",
    "    test_f1 = f1_score(\n",
    "        y_test_tensor, test_pred, pos_label=0\n",
    "    )  # F1 score for minority class\n",
    "    test_acc = accuracy_score(y_test_tensor, test_pred)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test F1 Score (minority class): {test_f1:.4f}\")\n",
    "    print(confusion_matrix(y_test_tensor, test_pred))\n",
    "    print(classification_report(y_test_tensor, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Model_SVM import SVMClassifierWrapper\n",
    "\n",
    "# # Choose the classifier\n",
    "# model_svm = SVMClassifierWrapper(C=1.0, kernel=\"rbf\", gamma=\"scale\", random_state=42)\n",
    "\n",
    "# # Convert data to numpy arrays (as these models do not use PyTorch tensors)\n",
    "# X_train_np = X_train\n",
    "# y_train_np = y_train\n",
    "# X_val_np = X_val\n",
    "# y_val_np = y_val\n",
    "# X_test_np = X_test\n",
    "# y_test_np = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training\n",
    "# model_svm.fit(X_train_np, y_train_np)\n",
    "\n",
    "# # # Evaluate on validation set\n",
    "# # val_pred = model_svm.predict(X_val_np)\n",
    "# # val_f1 = f1_score(y_val_np, val_pred, pos_label=0)  # F1 score for minority class\n",
    "\n",
    "# # Test the model\n",
    "# test_pred = model_svm.predict(X_test_np)\n",
    "# test_f1 = f1_score(y_test_np, test_pred, pos_label=0)  # F1 score for minority class\n",
    "# test_acc = accuracy_score(y_test_np, test_pred)\n",
    "# print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "# print(f\"Test F1 Score (minority class): {test_f1:.4f}\")\n",
    "# print(confusion_matrix(y_test_np, test_pred))\n",
    "# print(classification_report(y_test_np, test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
